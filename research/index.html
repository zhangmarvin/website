<!DOCTYPE HTML>

<html>
<head>
    <meta charset="utf-8">
    <title>Research - Marvin Zhang</title>
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="/css/style.css">
    <link rel="shortcut icon" href="/misc/favicon.ico">
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54050540-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
    <div id="header">
        <div class="home"><a class="link" href="/">marvin zhang</a></div>
        <div class="links">
            <a class="link" href="/teaching">teaching</a>&nbsp;/&nbsp;<a class="link" href="/contact">contact</a>
        </div>
    </div>
    <br>
    <h2>research</h2>
    <p>
    My work is focused on developing machine learning methods and algorithms
    that can allow robots discover and learn complex and intelligent behavior.
    Recently, I've been interested in unsupervised learning and optimal control
    methods. In
    <a class="link" href="https://arxiv.org/abs/1609.09049">my last project</a>,
    I developed a deep reinforcement learning method for learning locomotion for
    <a class="link" href="https://www.youtube.com/watch?v=EeLXhwNRrLQ">tensegrity robots</a>.
    In the past, I have explored learning deep control policies with memory
    capabilities, and I have also helped to develop frameworks for facilitating
    research efforts. For more details, refer to my
    <a class="link" href="/misc/cv.pdf">CV</a>.
    </p>
    <div id="publications">
        <h3>publications</h3>
        <table>
            <tr>
                <td>
                    <a href="https://arxiv.org/abs/1609.09049"><img style="height:125px" src="/misc/sup.png"/></a>
                </td>
                <td>
                    <a class="link" href="https://arxiv.org/abs/1609.09049">
                    <h4>Xinyang Geng*, Marvin Zhang*, Jonathan Bruce*, Ken Caluwaerts, Massimo Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine.
                    <b>Deep Reinforcement Learning for Tensegrity Robot Locomotion.</b>
                    <i>Under review at ICRA 2017. arXiv 1609.09049.</i></h4>
                    </a>
                    <p>
                    In this paper, we explore the challenges associated with
                    learning stable and efficient periodic locomotion, and we
                    develop novel extensions to the mirror descent guided policy
                    search algorithm to adapt the algorithm to this type of
                    domain. We are then able to learn successful locomotion for
                    the NASA SUPERball, a tensegrity robot with a number of
                    properties that make it a promising candidate for future
                    planetary exploration missions. Videos and supplementary
                    materials are available from the
                    <a class="link" href="http://rll.berkeley.edu/drl_tensegrity/">project website</a>.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <a href="http://arxiv.org/abs/1507.01273"><img src="/misc/rnn.png"/></a>
                </td>
                <td>
                    <a class="link" href="http://arxiv.org/abs/1507.01273">
                    <h4>Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel.
                    <b>Learning Deep Neural Network Policies with Continuous Memory States.</b>
                    <i>ICRA 2016. arXiv 1507.01273.</i></h4>
                    </a>
                    <p>
                    In this paper, we use the guided policy search algorithm to
                    train control policies with continuous memory states, which
                    can be understood as a type of recurrent neural network. We
                    show that this policy can successfully and efficiently
                    complete several simulated tasks that either require memory
                    or can be made easier by the use of memory. We compare our
                    method to several other baselines, and show that our method
                    outperforms all of these alternate approaches.
                    </p>
                </td>
            </tr>
        </table>
    </div>
    <div id="presentations">
        <h3>presentations</h3>
        <ul>
            <li>
                I gave a talk at
                <a class="link" href="https://nips.cc/Conferences/2015">NIPS 2015</a> in the
                <a class="link" href="http://www.thespermwhale.com/jaseweston/ram/">Reasoning, Attention, Memory (RAM) Workshop</a>
                on my recent work in training policies with memory.
                My slides are available
                <a class="link" href="http://www.thespermwhale.com/jaseweston/ram/slides/session4/ram_talk_zhang_marvin.pdf">here</a>.
                The workshop was
                <a class="link" href="https://www.facebook.com/photo.php?fbid=1626023934328466">packed to the brim</a>!
            </li>
            <li>
                I also gave a much shorter spotlight talk and poster
                presentation on the same work at the
                <a class="link" href="http://rll.berkeley.edu/deeprlworkshop/">Deep Reinforcement Learning Workshop</a>.
            </li>
        </ul>
    </div>
    <div id="software">
        <h3>software</h3>
        <ul>
            <li>
                <a class="link" href="http://rll.berkeley.edu/gps/">GPS</a> is a
                standard, open-source implementation of the guided policy search
                algorithm, developed by myself and several colleagues. The goal
                of this project is to allow other researchers to understand,
                use, and extend guided policy search in their own research.
            </li>
            <li>
                <a class="link" href="https://github.com/zhangmarvin/parRL">parRL</a>
                is a framework for parallelizing reinforcement learning across
                computing clusters that I developed alongside PhD student John
                Schulman. The main goals were for the framework to be
                lightweight, robust, simple, and scalable.
            </li>
        </ul>
    </div>
</body>
</html>
