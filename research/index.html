<!DOCTYPE HTML>

<html>
<head>
    <meta charset="utf-8">
    <title>Research - Marvin Zhang</title>
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="/css/style.css">
    <link rel="shortcut icon" href="/misc/favicon.ico">
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54050540-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
    <div id="header">
        <div class="home"><a class="link" href="/">marvin zhang</a></div>
        <div class="links">
            <a class="link" href="/teaching">teaching</a>&nbsp;/&nbsp;<a class="link" href="/contact">contact</a>
        </div>
    </div>
    <br>
    <h2>research</h2>
    <p>
    My work is focused on developing machine learning methods and algorithms
    that can allow robots to discover and learn complex and intelligent
    behavior.  Recently, I've been interested in representation learning and
    model-based reinforcement learning methods.
    </p>
    <div id="publications">
        <h3>publications</h3>
        <table>
            <tr>
                <td>
                    <br>
                    <a href="https://arxiv.org/abs/1906.08253"><img src="/misc/hop.png"/></a>
                </td>
                <td>
                    <h4>Michael Janner, Justin Fu, Marvin Zhang, Sergey Levine.<br>
                    When to Trust Your Model: Model-Based Policy Optimization.<br>
                    <i>NeurIPS 2019.</i></h4>
                    <a class="link" href="https://arxiv.org/abs/1906.08253">paper</a>&nbsp;/&nbsp;<a class="link" href="https://people.eecs.berkeley.edu/~janner/mbpo/">website</a>&nbsp;/&nbsp;<a class="link" href="https://github.com/JannerM/mbpo">code</a>&nbsp;/&nbsp;<a class="link" href="https://www.youtube.com/watch?v=rdF7q8MipRs">talk</a>
                    <p>
                    Designing effective model-based reinforcement learning
                    algorithms is difficult because the ease of data generation
                    must be weighed against the bias of model-generated data. In
                    this paper, we study the role of model usage in policy
                    optimization both theoretically and empirically. We first
                    formulate and analyze a model-based reinforcement learning
                    algorithm with a guarantee of monotonic improvement at each
                    step. In practice, this analysis is overly pessimistic and
                    suggests that real off-policy data is always preferable to
                    model-generated on-policy data, but we show that an
                    empirical estimate of model generalization can be
                    incorporated into such analysis to justify model usage.
                    Motivated by this analysis, we then demonstrate that a
                    simple procedure of using short model-generated rollouts
                    branched from real data has the benefits of more complicated
                    model-based algorithms without the usual pitfalls. In
                    particular, this approach surpasses the sample efficiency of
                    prior model-based methods, matches the asymptotic
                    performance of the best model-free algorithms, and scales to
                    horizons that cause other model-based methods to fail
                    entirely.
                </p>
                </td>
            </tr>
            <tr>
                <td>
                    <br>
                    <a href="https://arxiv.org/abs/1808.09105"><img src="/misc/sol.png"/></a>
                </td>
                <td>
                    <h4>Marvin Zhang*, Sharad Vikram*, Laura Smith, Pieter Abbeel, Matthew Johnson, Sergey Levine.<br>
                    SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning.<br>
                    <i>ICML 2019.</i></h4>
                    <a class="link" href="https://arxiv.org/abs/1808.09105">paper</a>&nbsp;/&nbsp;<a class="link" href="https://sites.google.com/corp/view/icml19solar">website</a>&nbsp;/&nbsp;<a class="link" href="https://github.com/sharadmv/parasol">code</a>&nbsp;/&nbsp;<a class="link" href="https://bair.berkeley.edu/blog/2019/05/20/solar/">blog</a>&nbsp;/&nbsp;<a class="link" href="https://youtu.be/pCbs80XWQaY">talk</a>
                    <p>
                    Model-based reinforcement learning (RL) has proven to be a
                    data efficient approach for learning control tasks but is
                    difficult to utilize in domains with complex observations
                    such as images. In this paper, we present a method for
                    learning representations that are suitable for iterative
                    model-based policy improvement, even when the underlying
                    dynamical system has complex dynamics and image
                    observations, in that these representations are optimized
                    for inferring simple dynamics and cost models given data
                    from the current policy. This enables a model-based RL
                    method based on the linear-quadratic regulator (LQR) to be
                    used for systems with image observations. We evaluate our
                    approach on a range of robotics tasks, including
                    manipulation with a real-world robotic arm directly from
                    images. We find that our method produces substantially
                    better final performance than other model-based RL methods
                    while being significantly more efficient than model-free RL.
                </p>
                </td>
            </tr>
            <tr>
                <td>
                    <br>
                    <a href="https://arxiv.org/abs/1703.03078"><img src="/misc/hoc.jpg"/></a>
                </td>
                <td>
                    <h4>Yevgen Chebotar*, Karol Hausman*, Marvin Zhang*, Gaurav Sukhatme, Stefan Schaal, Sergey Levine.<br>
                    <b>Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning.</b><br>
                    <i>ICML 2017.</i></h4>
                    <a class="link" href="https://arxiv.org/abs/1703.03078">paper</a>&nbsp;/&nbsp;<a class="link" href="https://sites.google.com/site/icml17pilqr/">website</a>&nbsp;/&nbsp;<a class="link" href="https://github.com/cbfinn/gps">code</a>
                    <p>
                    Reinforcement learning (RL) algorithms for real-world
                    robotic applications need a data-efficient learning process
                    and the ability to handle complex, unknown dynamical
                    systems. These requirements are handled well by model-based
                    and model-free RL approaches, respectively. In this work, we
                    aim to combine the advantages of these two types of methods
                    in a principled manner. By focusing on time-varying
                    linear-Gaussian policies, we enable a model-based algorithm
                    based on the linear quadratic regulator (LQR) that can be
                    integrated into the model-free framework of path integral
                    policy improvement (PI2). We can further combine our method
                    with guided policy search (GPS) to train arbitrary
                    parameterized policies such as deep neural networks. Our
                    simulation and real-world experiments demonstrate that this
                    method can solve challenging manipulation tasks with
                    comparable or better performance than model-free methods
                    while maintaining the sample efficiency of model-based
                    methods.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <br>
                    <a href="https://arxiv.org/abs/1609.09049"><img style="height:125px" src="/misc/sup.png"/></a>
                </td>
                <td>
                    <h4>Marvin Zhang*, Xinyang Geng*, Jonathan Bruce*, Ken Caluwaerts, Massimo Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine.<br>
                    <b>Deep Reinforcement Learning for Tensegrity Robot Locomotion.</b><br>
                    <i>ICRA 2017.</i></h4>
                    <a class="link" href="https://arxiv.org/abs/1609.09049">paper</a>&nbsp;/&nbsp;<a class="link" href="http://rll.berkeley.edu/drl_tensegrity/">website</a>&nbsp;/&nbsp;<a class="link" href="https://github.com/young-geng/gps_superball_public">code</a>
                    <p>
                    Tensegrity robots, composed of rigid rods connected by
                    elastic cables, have a number of unique properties that make
                    them appealing for use as planetary exploration rovers.
                    However, control of tensegrity robots remains a difficult
                    problem due to their unusual structures and complex
                    dynamics. In this work, we show how locomotion gaits can be
                    learned automatically using a novel extension of mirror
                    descent guided policy search (MDGPS) applied to periodic
                    locomotion movements, and we demonstrate the effectiveness
                    of our approach on tensegrity robot locomotion. We evaluate
                    our method with real-world and simulated experiments on the
                    SUPERball tensegrity robot, showing that the learned
                    policies generalize to changes in system parameters,
                    unreliable sensor measurements, and variation in
                    environmental conditions, including varied terrains and a
                    range of different gravities. Our experiments demonstrate
                    that our method not only learns fast, power-efficient
                    feedback policies for rolling gaits, but that these policies
                    can succeed with only the limited onboard sensing provided
                    by SUPERball's accelerometers. We compare the learned
                    feedback policies to learned open-loop policies and
                    hand-engineered controllers, and demonstrate that the
                    learned policy enables the first continuous, reliable
                    locomotion gait for the real SUPERball robot.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <br>
                    <a href="http://arxiv.org/abs/1507.01273"><img src="/misc/rnn.png"/></a>
                </td>
                <td>
                    <h4>Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel.<br>
                    <b>Learning Deep Neural Network Policies with Continuous Memory States.</b><br>
                    <i>ICRA 2016.</i></h4>
                    <a class="link" href="https://arxiv.org/abs/1507.01273">paper</a>
                    <p>
                    Policy learning for partially observed control tasks
                    requires policies that can remember salient information from
                    past observations. In this paper, we present a method for
                    learning policies with internal memory for high-dimensional,
                    continuous systems, such as robotic manipulators. Our
                    approach consists of augmenting the state and action space
                    of the system with continuous-valued memory states that the
                    policy can read from and write to. Learning general-purpose
                    policies with this type of memory representation directly is
                    difficult, because the policy must automatically figure out
                    the most salient information to memorize at each time step.
                    We show that, by decomposing this policy search problem into
                    a trajectory optimization phase and a supervised learning
                    phase through a method called guided policy search, we can
                    acquire policies with effective memorization and recall
                    strategies. Intuitively, the trajectory optimization phase
                    chooses the values of the memory states that will make it
                    easier for the policy to produce the right action in future
                    states, while the supervised learning phase encourages the
                    policy to use memorization actions to produce those memory
                    states. We evaluate our method on tasks involving continuous
                    control in manipulation and navigation settings, and show
                    that our method can learn complex policies that successfully
                    complete a range of tasks that require memory.
                    </p>
                </td>
            </tr>
        </table>
    </div>
    <div id="software">
        <h3>software</h3>
        <ul>
            <li>
                <a class="link" href="https://github.com/sharadmv/parasol">SOLAR</a>
                is the open-source implementation of the SOLAR algorithm,
		developed by myself,
		<a class="link" href="http://www.sharadvikram.com/">Sharad Vikram</a>,
                and Laura Smith.
	    </li>
        </ul>
        <ul>
            <li>
                <a class="link" href="https://github.com/cbfinn/gps">GPS</a> is a
                standard, open-source implementation of the guided policy search
                framework, developed by myself and several colleagues.
            </li>
        </ul>
    </div>
</body>
</html>
