<!DOCTYPE HTML>

<html>
<head>
    <meta charset="utf-8">
    <title>Research - Marvin Zhang</title>
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="/css/style.css">
    <link rel="shortcut icon" href="/misc/favicon.ico">
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54050540-1', 'auto');
  ga('send', 'pageview');

</script>
<body>
    <div id="header">
        <div class="home"><a class="link" href="/">marvin zhang</a></div>
        <div class="links">
            <a class="link" href="/teaching">teaching</a>&nbsp;/&nbsp;<a class="link" href="/contact">contact</a>
        </div>
    </div>
    <br>
    <h2>research</h2>
    <p>
    My work is focused on developing machine learning methods and algorithms
    that can allow robots to discover and learn complex and intelligent
    behavior.  Recently, I've been interested in representation learning and
    model-based reinforcement learning methods.
    </p>
    <div id="publications">
        <h3>publications</h3>
        <table>
            <tr>
                <td>
                    <a href="https://arxiv.org/abs/1808.09105"><img src="/misc/sol.png"/></a>
                </td>
                <td>
                    <a class="link" href="https://arxiv.org/abs/1808.09105">
                    <h4>Marvin Zhang*, Sharad Vikram*, Laura Smith, Pieter Abbeel, Matthew Johnson, Sergey Levine.
                    <b>SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning.</b>
                    <i>arXiv Preprint. arXiv 1808.09105.</i></h4>
                    </a>
                    <p>
                    In this paper, we develop a model-based reinforcement
                    learning method that utilizes structured representation
                    learning to find latent representations that are better
                    suited to accurate local linear model fitting. We show that
                    our method, which we call SOLAR, is similarly sample
                    efficient to model-based methods while solving complex tasks
                    to a comparable performance as model-free methods. We
                    demonstrate results in several simulated domains as well as
                    on a real Sawyer robotic arm, operating directly from only
                    image observations in all tasks. Videos of our results are
                    available on the
                    <a class="link" href="https://sites.google.com/view/icml19solar">project website</a>.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <a href="https://arxiv.org/abs/1703.03078"><img src="/misc/hoc.jpg"/></a>
                </td>
                <td>
                    <a class="link" href="https://arxiv.org/abs/1703.03078">
                    <h4>Yevgen Chebotar*, Karol Hausman*, Marvin Zhang*, Gaurav Sukhatme, Stefan Schaal, Sergey Levine.
                    <b>Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning.</b>
                    <i>ICML 2017. arXiv 1703.03078.</i></h4>
                    </a>
                    <p>
                    In this paper, we devise an algorithm for training
                    time-varying linear-Gaussian controllers that combines a
                    sample-efficient model-based method with a corrective
                    model-free method. We show that our algorithm, which we call
                    PILQR, retains the sample efficiency of model-based methods
                    while suffering less from modeling errors, which we
                    demonstrate through several simulated and real robot
                    experiments. We also combine PILQR with guided policy search
                    to train expressive and general deep neural network
                    policies. A video of our experimental results is available
                    from the
                    <a class="link" href="https://sites.google.com/site/icml17pilqr/">project website</a>.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <a href="https://arxiv.org/abs/1609.09049"><img style="height:125px" src="/misc/sup.png"/></a>
                </td>
                <td>
                    <a class="link" href="https://arxiv.org/abs/1609.09049">
                    <h4>Marvin Zhang*, Xinyang Geng*, Jonathan Bruce*, Ken Caluwaerts, Massimo Vespignani, Vytas SunSpiral, Pieter Abbeel, Sergey Levine.
                    <b>Deep Reinforcement Learning for Tensegrity Robot Locomotion.</b>
                    <i>ICRA 2017. arXiv 1609.09049.</i></h4>
                    </a>
                    <p>
                    In this paper, we explore the challenges associated with
                    learning stable and efficient periodic locomotion, and we
                    develop novel extensions to the mirror descent guided policy
                    search algorithm to better handle this type of domain. Our
                    method is able to learn successful locomotion for the NASA
                    SUPERball, a tensegrity robot with a number of properties
                    that make it a promising candidate for future planetary
                    exploration missions. Videos and supplementary materials are
                    available from the
                    <a class="link" href="http://rll.berkeley.edu/drl_tensegrity/">project website</a>.
                    </p>
                </td>
            </tr>
            <tr>
                <td>
                    <a href="http://arxiv.org/abs/1507.01273"><img src="/misc/rnn.png"/></a>
                </td>
                <td>
                    <a class="link" href="http://arxiv.org/abs/1507.01273">
                    <h4>Marvin Zhang, Zoe McCarthy, Chelsea Finn, Sergey Levine, Pieter Abbeel.
                    <b>Learning Deep Neural Network Policies with Continuous Memory States.</b>
                    <i>ICRA 2016. arXiv 1507.01273.</i></h4>
                    </a>
                    <p>
                    In this paper, we train control policies with continuous
                    memory states, which can be understood as a type of
                    recurrent neural network. We show that these policies can
                    successfully and efficiently complete several simulated
                    tasks that either require memory or can be made easier by
                    the use of memory. We compare our method to several other
                    baselines, and show that our method outperforms all of these
                    alternate approaches.
                    </p>
                </td>
            </tr>
        </table>
    </div>
    <div id="presentations">
        <h3>presentations</h3>
        <ul>
            <li>
                I presented at the
                <a class="link" href="https://nips.cc/Conferences/2015">NIPS 2015</a>
                <a class="link" href="http://www.thespermwhale.com/jaseweston/ram/">Reasoning, Attention, Memory (RAM)</a>
                workshop on my work on training policies with memory.
                My slides are available
                <a class="link" href="http://www.thespermwhale.com/jaseweston/ram/slides/session4/ram_talk_zhang_marvin.pdf">here</a>.
                The workshop was
                <a class="link" href="https://www.facebook.com/photo.php?fbid=1626023934328466">packed to the brim</a>!
            </li>
        </ul>
    </div>
    <div id="software">
        <h3>software</h3>
        <ul>
            <li>
                <a class="link" href="http://rll.berkeley.edu/gps/">GPS</a> is a
                standard, open-source implementation of the guided policy search
                framework, developed by myself and several colleagues. The goal
                of this project is to allow other researchers to understand,
                use, and extend guided policy search in their own research.
            </li>
        </ul>
    </div>
</body>
</html>
